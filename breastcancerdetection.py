# -*- coding: utf-8 -*-
"""Copie de Copie de BreastCancerDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lYyBLZXd0OFpucepusKTFuYqCh64ACiQ
"""

import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler



# Charger le dataset
df = pd.read_csv("data/data.csv")

print("=== √âTAPE 0 : APER√áU DU DATASET ===")
print(f"Nombre de lignes : {df.shape[0]}")
print(f"Nombre de colonnes : {df.shape[1]}")
print("\nPremi√®res lignes :")
print(df.head())
print("\nListe des colonnes :")
print(df.columns.tolist())
print("\n" + "="*60 + "\n")

# =========================
# 1) Valeurs nulles (une seule fois)
# =========================
print("=== √âTAPE 1 : VALEURS NULLES DANS LE DATASET ===")
null_counts = df.isna().sum()
print(null_counts)
print("\nColonnes avec au moins 1 valeur nulle :")
print(null_counts[null_counts > 0])
print("\n" + "="*60 + "\n")

# =========================
# 2) Suppression de 'id' et des colonnes compl√®tement vides
# =========================
print("=== √âTAPE 2 : NETTOYAGE DES COLONNES INUTILES ===")

if "id" in df.columns:
    print("‚Üí Suppression de la colonne 'id' (identifiant, non pertinente pour le mod√®le).")
    df = df.drop(columns=["id"])

colonnes_vides = [col for col in df.columns if df[col].isna().sum() == len(df)]
if colonnes_vides:
    print("‚Üí Colonnes compl√®tement vides supprim√©es :", colonnes_vides)
    df = df.drop(columns=colonnes_vides)
else:
    print("‚Üí Aucune colonne compl√®tement vide trouv√©e.")

print("Colonnes restantes :")
print(df.columns.tolist())
print("\n" + "="*60 + "\n")

# =========================
# 3) Encodage de la cible 'diagnosis'
# =========================
print("=== √âTAPE 3 : ENCODAGE DE 'diagnosis' ===")
if "diagnosis" not in df.columns:
    raise ValueError("La colonne 'diagnosis' est absente du dataset.")

print("Valeurs uniques de 'diagnosis' AVANT encodage :")
print(df["diagnosis"].unique())

df["diagnosis"] = df["diagnosis"].map({"M": 1, "B": 0})

print("\nQuelques valeurs de 'diagnosis' APR√àS encodage :")
print(df["diagnosis"].head())
print("\n" + "="*60 + "\n")

# =========================
# 4) S√©paration X / y
# =========================
print("=== √âTAPE 4 : S√âPARATION X / y ===")
y = df["diagnosis"]
X = df.drop(columns=["diagnosis"])

print(f"Shape de X : {X.shape}")
print(f"Shape de y : {y.shape}")
print("\nAper√ßu de X (5 premi√®res lignes) :")
print(X.head())
print("\n" + "="*60 + "\n")

# =========================
# 5) Imputation des valeurs manquantes (m√©diane)
# =========================
print("=== √âTAPE 5 : IMPUTATION DES VALEURS MANQUANTES ===")
print("Somme des valeurs nulles dans X AVANT imputation :")
print(X.isna().sum())

# Imputation par m√©diane
X = X.fillna(X.median(numeric_only=True))

print("\nSomme des valeurs nulles dans X APR√àS imputation :")
print(X.isna().sum())
print("\n" + "="*60 + "\n")

# =========================
# 6) D√©tection et affichage des valeurs aberrantes (IQR)
# =========================
print("=== √âTAPE 6 : D√âTECTION DES VALEURS ABERRANTES (OUTLIERS) ===")

outliers_counts = {}
outlier_mask_global = pd.Series(False, index=X.index)

for col in X.columns:
    Q1 = X[col].quantile(0.25)
    Q3 = X[col].quantile(0.75)
    IQR = Q3 - Q1
    low_lim = Q1 - 1.5 * IQR
    high_lim = Q3 + 1.5 * IQR

    col_mask = (X[col] < low_lim) | (X[col] > high_lim)
    outliers_counts[col] = col_mask.sum()

    # on marque les lignes qui ont au moins un outlier
    outlier_mask_global = outlier_mask_global | col_mask

# Affichage du nombre d'outliers par colonne (tri√© d√©croissant)
outliers_counts = pd.Series(outliers_counts).sort_values(ascending=False)
print("Nombre d'outliers par colonne :")
print(outliers_counts)

nb_lignes_outliers = outlier_mask_global.sum()
print(f"\nNombre de lignes contenant au moins un outlier : {nb_lignes_outliers}")

print("\nAper√ßu de quelques lignes contenant au moins un outlier :")
print(X[outlier_mask_global].head())
print("\n(‚ö†Ô∏è Aucune suppression des outliers n'est effectu√©e, ils sont seulement d√©tect√©s et affich√©s.)")
print("\n" + "="*60 + "\n")

# =========================
# 7) Normalisation (StandardScaler)
# =========================
print("=== √âTAPE 7 : NORMALISATION DES FEATURES ===")
scaler = StandardScaler()
X_scaled_array = scaler.fit_transform(X)
X_scaled = pd.DataFrame(X_scaled_array, columns=X.columns)

print("Aper√ßu de X normalis√© (valeurs arrondies) :")
print(X_scaled.round(3).head())
print("\n" + "="*60 + "\n")

# √Ä ce stade :
# - X : features nettoy√©es + imput√©es (avec outliers conserv√©s)
# - X_scaled : version normalis√©e de X
# - y : cible encod√©e (0 = B, 1 = M)

"""# SOFTMAX


Optimiseur :
‚Üí Adam

Fonction de perte :
‚Üí Cross-entropy (categorical cross-entropy), comme d√©fini dans l‚Äô√©quation (15).

Pr√©traitement :
‚Üí Standardisation des donn√©es via StandardScaler.

Architecture du mod√®le :
‚Üí Une couche lin√©aire Dense avec 2 neurones, activation Softmax (mod√®le Softmax Regression).

D√©coupage des donn√©es :
‚Üí 70% pour l‚Äôentra√Ænement et 30% pour le test, conform√©ment √† l‚Äôarticle.

M√©triques utilis√©es :
‚Üí Accuracy, TPR, TNR, FPR et FNR, comme dans la section ‚ÄúResults & Discussion
"""

# ============================================
# 8) Softmax Regression (mod√®le lin√©aire)
# ============================================
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

# 1) Split train / test (comme dans l‚Äôarticle : 70/30)
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled.values,
    y.values,
    test_size=0.3,
    random_state=42,
    stratify=y
)

# 2) One-hot encoding des labels pour Softmax (2 classes : 0 et 1)
y_train_cat = to_categorical(y_train, num_classes=2)
y_test_cat  = to_categorical(y_test, num_classes=2)

# 3) Mod√®le Softmax Regression :
#    - une couche Dense lin√©aire
#    - 2 neurones (2 classes)
#    - activation 'softmax'
model = Sequential([
    Dense(2, activation='softmax', input_shape=(X_train.shape[1],))
])

# 4) Compilation du mod√®le
#    - perte : categorical_crossentropy (cross-entropy comme dans l‚Äôarticle)
#    - optimiseur : Adam (comme mentionn√© pour GRU-SVM, Softmax, SVM)
optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
model.compile(optimizer=optimizer,
              loss='categorical_crossentropy',
              metrics=['accuracy'])

# 5) Entra√Ænement
history = model.fit(
    X_train, y_train_cat,
    epochs=3000,
    batch_size=128,
    validation_split=0.2,
    verbose=1
)

# 6) √âvaluation sur le test set
test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"Test accuracy (Softmax Regression) : {test_acc:.4f}")

# 7) Exemple de pr√©diction de probabilit√© de malignit√© sur quelques √©chantillons
probas = model.predict(X_test[:5])
print("\nProbabilit√©s pr√©dite [p(B√©nin), p(Malin)] pour les 5 premiers exemples :")
print(probas)
print("\nClasse pr√©dite (argmax) :")
print(probas.argmax(axis=1))
print("Classe r√©elle :")
print(y_test[:5])

# ============================================================
# DIGITAL TWIN ‚Äì SIMULATION D'√âVOLUTION CELLULAIRE
# (√Ä coller apr√®s ton pr√©-traitement : X_scaled, y)
# ============================================================

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.decomposition import PCA

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.utils import to_categorical

import imageio.v2 as imageio
from IPython.display import Image as IPyImage, display

# ------------------------------------------------------------
# 1) V√©rification des donn√©es d'entr√©e
#    On suppose que :
#    - X_scaled : DataFrame normalis√© (StandardScaler)
#    - y        : Series (0 = b√©nin, 1 = malin)
# ------------------------------------------------------------
print("Shape X_scaled :", X_scaled.shape)
print("R√©partition des classes dans y :")
print(y.value_counts())

# ------------------------------------------------------------
# 2) Entra√Ænement du mod√®le Softmax (comme dans l‚Äôarticle)
# ------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled.values,
    y.values,
    test_size=0.3,
    random_state=42,
    stratify=y
)

y_train_cat = to_categorical(y_train, num_classes=2)
y_test_cat  = to_categorical(y_test, num_classes=2)

model = Sequential([
    Dense(2, activation='softmax', input_shape=(X_train.shape[1],))
])

model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['accuracy']
)

history = model.fit(
    X_train, y_train_cat,
    epochs=100,
    batch_size=32,
    validation_split=0.2,
    verbose=0
)

test_loss, test_acc = model.evaluate(X_test, y_test_cat, verbose=0)
print(f"\nAccuracy test (Softmax Regression) : {test_acc:.4f}")

# ------------------------------------------------------------
# 3) Pr√©paration Digital Twin :
#    - centro√Ødes b√©nin / malin dans l'espace normalis√©
# ------------------------------------------------------------
X_df = X_scaled.copy()
y_series = y.copy()

X_benign     = X_df[y_series == 0]
X_malignant  = X_df[y_series == 1]

mu_benign    = X_benign.mean(axis=0).values
mu_malignant = X_malignant.mean(axis=0).values

print("\nCentroides calcul√©s (b√©nin / malin).")

# ------------------------------------------------------------
# 4) Fonction de simulation pour un patient b√©nin
# ------------------------------------------------------------
def simulate_trajectory(idx, X_df, mu_malignant, model, n_steps=21):
    """
    Simule la trajectoire d'une cellule b√©nigne vers le centro√Øde malin.

    idx          : index (position) du patient dans X_df (b√©nin de pr√©f√©rence)
    X_df         : DataFrame (features normalis√©es)
    mu_malignant : vecteur moyen des tumeurs malignes (centro√Øde)
    model        : mod√®le Softmax d√©j√† entra√Æn√©
    n_steps      : nombre de points le long de la trajectoire (t ‚àà [0,1])
    """
    x0 = X_df.iloc[idx].values
    ts = np.linspace(0, 1, n_steps)

    traj = []
    probas_malin = []

    for t in ts:
        # Interpolation lin√©aire entre l'√©tat b√©nin initial et le centro√Øde malin
        x_t = (1 - t) * x0 + t * mu_malignant
        traj.append(x_t)

        # Probabilit√© pr√©dite d'√™tre malin (classe 1)
        p = model.predict(x_t.reshape(1, -1), verbose=0)[0, 1]
        probas_malin.append(p)

    traj = np.array(traj)
    probas_malin = np.array(probas_malin)
    return ts, traj, probas_malin

# ------------------------------------------------------------
# 5) Choix d‚Äôun patient b√©nin pour illustrer la trajectoire
# ------------------------------------------------------------
benign_indices = np.where(y_series.values == 0)[0]
idx_example = benign_indices[0]   # tu peux changer pour un autre index b√©nin
print(f"\nPatient b√©nin choisi (index global) : {idx_example}")

ts, traj, probas_malin = simulate_trajectory(
    idx_example, X_df, mu_malignant, model, n_steps=21
)

# ------------------------------------------------------------
# 6) VISU 1 : Courbe d‚Äô√©volution du risque de malignit√©
# ------------------------------------------------------------
plt.figure(figsize=(8,5))
plt.plot(ts, probas_malin, marker='o')
plt.axhline(0.5, linestyle='--', color='red', label='Seuil 0.5')
plt.title("√âvolution du risque de malignit√©\nDigital Twin d‚Äôune cellule b√©nigne ‚Üí maligne")
plt.xlabel("t (0 = √©tat b√©nin initial, 1 = centro√Øde malin)")
plt.ylabel("Probabilit√© pr√©dite de malignit√©")
plt.ylim(0, 1.02)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

# ------------------------------------------------------------
# 7) VISU 2 : Trajectoires de plusieurs patientes b√©nignes
# ------------------------------------------------------------
plt.figure(figsize=(8,5))

n_examples = 10  # nombre de patientes b√©nignes √† simuler
for idx in benign_indices[:n_examples]:
    ts_i, traj_i, probas_i = simulate_trajectory(idx, X_df, mu_malignant, model, n_steps=21)
    plt.plot(ts_i, probas_i, alpha=0.35)

plt.axhline(0.5, linestyle='--', color='red', label='Seuil 0.5')
plt.title("Superposition des trajectoires de risque\npour plusieurs cellules b√©nignes")
plt.xlabel("t (0 = b√©nin, 1 = centro√Øde malin)")
plt.ylabel("Probabilit√© pr√©dite de malignit√©")
plt.ylim(0, 1.02)
plt.grid(True, alpha=0.3)
plt.legend()
plt.show()

# ------------------------------------------------------------
# 8) VISU 3 : Projection PCA 2D avec trajectoire b√©nin ‚Üí malin
# ------------------------------------------------------------
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X_df.values)

# Coordonn√©es du centro√Øde malin, de la cellule b√©nigne et de la trajectoire
mu_malignant_pca = pca.transform(mu_malignant.reshape(1, -1))[0]
x0_pca           = pca.transform(X_df.iloc[idx_example].values.reshape(1, -1))[0]
traj_pca         = pca.transform(traj)

plt.figure(figsize=(7,7))

# Nuage de points global
plt.scatter(
    X_pca[y_series.values==0, 0],
    X_pca[y_series.values==0, 1],
    alpha=0.25, label='B√©nin', color='tab:blue', s=20
)
plt.scatter(
    X_pca[y_series.values==1, 0],
    X_pca[y_series.values==1, 1],
    alpha=0.25, label='Malin', color='tab:orange', s=20
)

# Trajectoire du digital twin
plt.plot(traj_pca[:,0], traj_pca[:,1], '-o', color='black', linewidth=2, label='Trajectoire Digital Twin')

# Points cl√© : √©tat initial, centro√Øde malin
plt.scatter(x0_pca[0], x0_pca[1], color='green', s=100, marker='o', label='Cellule b√©nigne initiale')
plt.scatter(mu_malignant_pca[0], mu_malignant_pca[1], color='red', s=120, marker='X', label='Centro√Øde malin')

plt.title("Digital Twin dans l‚Äôespace PCA 2D\nTrajectoire b√©nin ‚Üí malin")
plt.xlabel("PC1")
plt.ylabel("PC2")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# ------------------------------------------------------------
# 9) VISU 4 : Top 10 features qui √©voluent le plus
# ------------------------------------------------------------
delta = mu_malignant - X_df.iloc[idx_example].values
delta_abs = np.abs(delta)
top_idx = np.argsort(-delta_abs)[:10]  # top 10

top_features = X_df.columns[top_idx]
top_values   = delta[top_idx]

plt.figure(figsize=(10,5))
plt.barh(top_features, top_values)
plt.axvline(0, color='black')
plt.title("Top 10 caract√©ristiques qui √©voluent le plus\nb√©nin initial ‚Üí centro√Øde malin")
plt.xlabel("Variation (dans l‚Äôespace normalis√©)")
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.show()

# ------------------------------------------------------------
# 10) GIF : Simulation visuelle de la cellule (digital twin)
# ------------------------------------------------------------
frames = []
filenames = []

for i, (t_val, p_malin) in enumerate(zip(ts, probas_malin)):
    # Plus la probabilit√© de malignit√© est √©lev√©e,
    # plus la cellule est grande et irr√©guli√®re.
    base_radius = 1.0 + 0.5 * p_malin          # rayon moyen
    irregularity = 0.15 + 0.55 * p_malin       # amplitude de d√©formation
    lobes = 6                                  # nombre de "bosses" sur le contour

    theta = np.linspace(0, 2*np.pi, 400)
    # Rayon en fonction de l‚Äôangle : cercle + perturbation sinus
    r = base_radius * (1 + irregularity * np.sin(lobes * theta) * np.cos(2 * theta))

    x = r * np.cos(theta)
    y = r * np.sin(theta)

    fig, ax = plt.subplots(figsize=(4,4))
    ax.fill(x, y, alpha=0.7)
    ax.set_aspect('equal')
    ax.set_xlim(-2, 2)
    ax.set_ylim(-2, 2)
    ax.axis('off')

    ax.set_title(
        f"t = {t_val:.2f}  |  p(malin) = {p_malin:.2f}",
        fontsize=10,
        pad=8
    )

    filename = f"frame_cell_{i:02d}.png"
    filenames.append(filename)
    plt.savefig(filename, bbox_inches='tight', pad_inches=0.1)
    plt.close(fig)

for filename in filenames:
    frames.append(imageio.imread(filename))

gif_filename = "cell_digital_twin.gif"
imageio.mimsave(gif_filename, frames, duration=0.25)  # 0.25 s entre images

print(f"GIF cr√©√© : {gif_filename}")
display(IPyImage(filename=gif_filename))

"""# L2-SVM (hinge loss)


Optimiseur : ‚Üí LinearSVC utilise un solveur interne bas√© sur l‚Äôoptimisation par descente du gradient sur la fonction objectif du L2-SVM.

Fonction de perte : ‚Üí Squared hinge loss (diff√©rentiable), correspondant au L2-SVM, comme d√©fini dans l‚Äô√©quation (19). Cette fonction vise √† maximiser la marge tout en p√©nalisant fortement les points mal class√©s.

Pr√©traitement : ‚Üí Standardisation des donn√©es via StandardScaler, pour que toutes les features aient une moyenne nulle et une variance unitaire, ce qui est crucial pour la convergence du SVM.

Architecture du mod√®le : ‚Üí Hyperplan lin√©aire
ùëì
(
ùë•
)
=
ùë§
ùëá
ùë•
+
ùëè
f(x)=w
T
x+b s√©parant les deux classes. Le mod√®le apprend un vecteur de poids
ùë§
w et un biais
ùëè
b pour maximiser la marge entre les points b√©nins et malins.

D√©coupage des donn√©es : ‚Üí 70‚ÄØ% pour l‚Äôentra√Ænement et 30‚ÄØ% pour le test, conform√©ment √† l‚Äôarticle.

M√©triques utilis√©es : ‚Üí Accuracy, TPR, TNR, FPR et FNR
"""

# ============================================
# SVM LIN√âAIRE L2 (LinearSVC squared_hinge)
# ============================================
from sklearn.svm import LinearSVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

print("=== SVM LIN√âAIRE L2 (LinearSVC squared_hinge) ===")

# ------------------------------------------------------------
# 0) Assurer que X_scaled et y correspondent
# ------------------------------------------------------------
# y provient du DataFrame initial df
y = df['diagnosis']  # 569 lignes apr√®s encodage
# R√©indexer X_scaled pour correspondre √† y
X_scaled = X_scaled.loc[y.index]

print("Shape X_scaled :", X_scaled.shape)
print("Shape y :", y.shape)

# ------------------------------------------------------------
# 1) Split train/test (70/30)
# ------------------------------------------------------------
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled.values,
    y.values,
    test_size=0.3,
    random_state=42,
    stratify=y
)

print(f"Shape X_train: {X_train.shape}, Shape X_test: {X_test.shape}")
print("R√©partition classes y_train:", np.unique(y_train, return_counts=True))

# ------------------------------------------------------------
# 2) D√©finition du mod√®le L2-SVM
# ------------------------------------------------------------
svm_model = LinearSVC(
    loss='squared_hinge',
    penalty='l2',
    C=5,
    max_iter=5000,
    random_state=42
)

# Entra√Ænement
print("\nEntra√Ænement du mod√®le SVM...")
svm_model.fit(X_train, y_train)

# ------------------------------------------------------------
# 3) Pr√©dictions et m√©triques
# ------------------------------------------------------------
y_pred = svm_model.predict(X_test)

accuracy = accuracy_score(y_test, y_pred)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

fpr = fp / (fp + tn)
fnr = fn / (fn + tp)
tpr = tp / (tp + fn)
tnr = tn / (tn + fp)

print(f"\nAccuracy test: {accuracy:.4f}")
print("\n=== Matrice de confusion ===")
print(cm)
print("\n=== M√©triques ===")
print(f"FPR: {fpr:.4f}, FNR: {fnr:.4f}, TPR: {tpr:.4f}, TNR: {tnr:.4f}")
print("\n=== Rapport de classification ===")
print(classification_report(y_test, y_pred, target_names=['B√©nin','Malin']))

# ------------------------------------------------------------
# 4) Visualisation PCA 2D + marges
# ------------------------------------------------------------
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)
X_test_pca = pca.transform(X_test)

svm_pca = LinearSVC(loss='squared_hinge', penalty='l2', C=5, max_iter=5000, random_state=42)
svm_pca.fit(X_train_pca, y_train)

# Cr√©ation de la meshgrid pour la fronti√®re de d√©cision
x_min, x_max = X_train_pca[:, 0].min() - 1, X_train_pca[:, 0].max() + 1
y_min, y_max = X_train_pca[:, 1].min() - 1, X_train_pca[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                     np.arange(y_min, y_max, 0.02))
Z = svm_pca.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.figure(figsize=(10,8))
plt.contourf(xx, yy, Z, alpha=0.3, cmap=plt.cm.coolwarm)
plt.scatter(X_train_pca[y_train==0, 0], X_train_pca[y_train==0, 1], c='blue', label='B√©nin', alpha=0.6, edgecolors='k')
plt.scatter(X_train_pca[y_train==1, 0], X_train_pca[y_train==1, 1], c='red', label='Malin', alpha=0.6, edgecolors='k')

# Fronti√®re de d√©cision + marges
w = svm_pca.coef_[0]
b = svm_pca.intercept_[0]
x_plot = np.linspace(x_min, x_max, 100)
y_plot = -(w[0]*x_plot + b)/w[1]
margin = 1 / np.linalg.norm(w)
plt.plot(x_plot, y_plot, 'k-', linewidth=2, label='Fronti√®re')
plt.plot(x_plot, y_plot + margin, 'k--', alpha=0.5, linewidth=1, label='Marge')
plt.plot(x_plot, y_plot - margin, 'k--', alpha=0.5, linewidth=1)

plt.xlabel('PC1')
plt.ylabel('PC2')
plt.title('L2-SVM - Fronti√®re de d√©cision et marges (PCA 2D)')
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

# ------------------------------------------------------------
# 5) Analyse des coefficients (feature importance)
# ------------------------------------------------------------
feature_importance = np.abs(svm_model.coef_[0])
importance_df = pd.DataFrame({
    'feature': X_scaled.columns,
    'importance': feature_importance
}).sort_values('importance', ascending=False)

print("Top 10 caract√©ristiques importantes:")
print(importance_df.head(10))

plt.figure(figsize=(10,6))
top_features = importance_df.head(10)
plt.barh(top_features['feature'], top_features['importance'])
plt.xlabel("Importance absolue des coefficients")
plt.title("Top 10 caract√©ristiques - L2-SVM")
plt.gca().invert_yaxis()
plt.grid(axis='x', alpha=0.3)
plt.show()

"""**K-Nearest Neighbors Classification on Breast Cancer Dataset**

In this section, we implement a K-Nearest Neighbors (KNN) classifier to predict whether a tumor is benign or malignant based on its features. The workflow includes splitting the data into training and testing sets. We then train the KNN model, evaluate its performance using standard classification metrics, and visualize the results with confusion matrices to assess how well the model distinguishes between benign and malignant tumors.
"""

from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import confusion_matrix

# L1-NN
knn_l1 = KNeighborsClassifier(n_neighbors=1, metric='manhattan')
knn_l1.fit(X_train, y_train)
y_pred = knn_l1.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
print("KNN L1")
print("Accuracy:", (tp+tn)/(tp+tn+fp+fn))

# L2-NN
knn_l2 = KNeighborsClassifier(n_neighbors=1, metric='euclidean')
knn_l2.fit(X_train, y_train)
y_pred = knn_l2.predict(X_test)
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()
print("KNN L2")
print("Accuracy:", (tp+tn)/(tp+tn+fp+fn))

"""#MLP ‚Äì Multilayer Perceptron



*   **Optimiseur (solver)** : ‚Üí **Adam**, un optimiseur de descente du gradient adaptatif qui ajuste automatiquement le learning rate √† chaque it√©ration.
*   **Fonction de perte** : **cross-entropy**, adapt√©e √† la classification binaire**(Benign/Malignant)**

*   D√©coupage des donn√©es: ‚Üí 70 % entra√Ænement / 30 % test, comme dans l‚Äôarticle.

*   Pr√©traitement: ‚Üí StandardScaler : mise √† l‚Äô√©chelle indispensable pour stabiliser Adam et permettre une convergence rapide des couches ReLU, car les features du dataset ont des amplitudes tr√®s diff√©rentes.
#  Architecture du mod√®le
Architecture MLP : 3 couches dans la Hidden_layer avec 500 neurones chacune
Fonction d'activation : ReLU
1 neurone de sortie avec probabilit√© (sigmo√Øde).

*   M√©triques utilis√©es: ‚Üí Accuracy, Recall (TPR), Specificity (TNR), FPR, FNR, F1-score, AUC ROC.
"""



import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score,
    roc_auc_score, roc_curve, confusion_matrix, classification_report
)

from tensorflow.keras import models, layers
from tensorflow.keras.optimizers import SGD  # Optimiseur remplac√© par SGD


# =====================================================
# 1) DATA SPLIT (70% train / 30% test)
# =====================================================
RANDOM_STATE = 42
TEST_SIZE = 0.30

X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y,
    test_size=TEST_SIZE,
    random_state=RANDOM_STATE,
    stratify=y
)

X_train = np.array(X_train)
X_test = np.array(X_test)

print("\n=== SPLIT DATA ===")
print("X_train :", X_train.shape)
print("X_test  :", X_test.shape)
print("y_train :", y_train.shape)
print("y_test  :", y_test.shape)


# =====================================================
# 2) BUILD MLP WITHOUT DROPOUT + L2
# =====================================================
model = models.Sequential([
    layers.Input(shape=(X_train.shape[1],)),

    layers.Dense(500, activation='relu'),
    layers.Dense(500, activation='relu'),
    layers.Dense(500, activation='relu'),

    layers.Dense(1, activation='sigmoid')
])

# Utilisation de l'optimiseur SGD (comme sp√©cifi√© dans l'article)
optimizer = SGD(learning_rate=0.01)  # SGD avec un learning rate classique de 0.01

model.compile(
    optimizer=optimizer,
    loss='binary_crossentropy',  # Cross-entropy pour la classification binaire
    metrics=['accuracy']
)

model.summary()


# =====================================================
# 3) EARLY STOPPING (Supprim√©)
# =====================================================
# EarlyStopping est supprim√© pour √™tre conforme √† l'article


# =====================================================
# 4) TRAINING
# =====================================================
history = model.fit(
    X_train, y_train,
    validation_split=0.2,
    epochs=3000,
    batch_size=128,
    verbose=1
)



# =====================================================
# 5) COURBE DE LOSS
# =====================================================
plt.plot(history.history['loss'], label='Train Loss')
plt.plot(history.history['val_loss'], label='Validation Loss')
plt.title('Cross-Entropy Loss Over Epochs (MLP sans r√©gularisation)')
plt.xlabel('Epochs')
plt.ylabel('Loss')
plt.legend()
plt.grid(True)
plt.show()


# =====================================================
# 6) PREDICTION
# =====================================================
y_train_pred = (model.predict(X_train) >= 0.5).astype(int).flatten()
y_test_pred = (model.predict(X_test) >= 0.5).astype(int).flatten()
y_test_proba = model.predict(X_test).flatten()


# =====================================================
# 7) M√âTRIQUES
# =====================================================
train_acc = accuracy_score(y_train, y_train_pred)
test_acc  = accuracy_score(y_test, y_test_pred)
prec = precision_score(y_test, y_test_pred)
rec = recall_score(y_test, y_test_pred)
f1 = f1_score(y_test, y_test_pred)
auc = roc_auc_score(y_test, y_test_proba)

print("\n=== METRIQUES ===")
print(f"Train Accuracy : {train_acc:.4f}")
print(f"Test Accuracy  : {test_acc:.4f}")
print(f"Precision      : {prec:.4f}")
print(f"Recall         : {rec:.4f}")
print(f"F1-score       : {f1:.4f}")
print(f"AUC ROC        : {auc:.4f}")

print("\nClassification Report :\n")
print(classification_report(y_test, y_test_pred))


# =====================================================
# 8) CONFUSION MATRIX
# =====================================================
cm = confusion_matrix(y_test, y_test_pred)
plt.figure(figsize=(5,4))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title("Confusion Matrix - MLP without regularization")
plt.xlabel("Predicted")
plt.ylabel("Actual")
plt.show()

tn, fp, fn, tp = cm.ravel()
specificity = tn / (tn + fp)
fpr = fp / (fp + tn)
fnr = fn / (fn + tp)

print(f"\nSpecificity (TNR): {specificity:.4f}")
print(f"FPR              : {fpr:.4f}")
print(f"FNR              : {fnr:.4f}")


# =====================================================
# 9) ROC CURVE
# =====================================================
fpr_curve, tpr_curve, _ = roc_curve(y_test, y_test_proba)
plt.figure(figsize=(6,6))
plt.plot(fpr_curve, tpr_curve, label=f"AUC = {auc:.3f}")
plt.plot([0, 1], [0, 1], '--', color='gray')
plt.title("ROC Curve - MLP")
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.grid(True)
plt.legend()
plt.show()

"""REGRESSION LINEAIRE"""

from sklearn.linear_model import SGDRegressor
from sklearn.metrics import mean_squared_error, confusion_matrix, classification_report

print("=== LINEAR REGRESSION CLASSIFIER (Section 2.4.2) ===")

# --- 1) Mod√®le Linear Regression avec SGD ---
linreg = SGDRegressor(
    loss='squared_error',
    penalty=None,
    max_iter=3000,
    learning_rate='constant',
    eta0=0.001,
    random_state=42
)


# --- 2) Entra√Ænement ---
linreg.fit(X_train, y_train)

# --- 3) Pr√©diction continue (Eq. 7) ---
y_pred_continuous = linreg.predict(X_test)

# --- 4) Seuil 0.5 (Eq. 8) ---
y_pred = (y_pred_continuous >= 0.5).astype(int)

# --- 5) MSE (Eq. 9) ---
mse = mean_squared_error(y_test, y_pred_continuous)
print(f"MSE: {mse:.4f}")

# --- 6) Matrice de confusion ---
cm = confusion_matrix(y_test, y_pred)
tn, fp, fn, tp = cm.ravel()

# M√©triques comme l‚Äôarticle : TPR, TNR, FPR, FNR
fpr = fp / (fp + tn)
fnr = fn / (fn + tp)
tpr = tp / (tp + fn)
tnr = tn / (tn + fp)

print("\n=== MATRICE DE CONFUSION ===")
print(cm)

print("\n=== METRIQUES ===")
print(f"Accuracy: {(tp+tn)/(tp+tn+fp+fn):.4f}")
print(f"TPR: {tpr:.4f}")
print(f"TNR: {tnr:.4f}")
print(f"FPR: {fpr:.4f}")
print(f"FNR: {fnr:.4f}")

print("\n=== RAPPORT DE CLASSIFICATION ===")
print(classification_report(y_test, y_pred, target_names=['Benign','Malignant']))

"""# **GRU-SVM**
Un GRU-SVM est un mod√®le hybride qui combine une couche GRU avec un classificateur SVM lin√©aire.
Le GRU (Gated Recurrent Unit) extrait automatiquement les caract√©ristiques importantes d‚Äôune s√©quence de donn√©es, tandis que le SVM lin√©aire plac√© en sortie r√©alise la classification en maximisant la marge entre les classes.

Ainsi, le GRU-SVM profite √† la fois de la capacit√© des GRU √† mod√©liser les d√©pendances dans les donn√©es s√©quentielles et de la robustesse du SVM pour effectuer une classification pr√©cise.
"""

# =============================
# 8) Pr√©paration des donn√©es pour GRU-SVM
# =============================
import numpy as np
from sklearn.model_selection import train_test_split
from tensorflow.keras import layers, regularizers, Model, Input
import tensorflow as tf
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from tensorflow.keras.losses import SquaredHinge

print("=== √âTAPE 8 : PR√âPARATION POUR GRU-SVM ===")

# 1) Conversion en numpy
X_np = X_scaled.values
y_np = y.values

# 2) SVM doit utiliser labels : +1 (malignant) / -1 (benign)
y_svm = np.where(y_np == 1, 1, -1).astype("float32")

# 3) Reshape pour GRU ‚Üí (N, 30, 1)
X_seq = X_np.reshape((X_np.shape[0], X_np.shape[1], 1))
print("Nouvelle forme de X pour GRU :", X_seq.shape)

# 4) Split train/test (70/30 comme article)
X_train, X_test, y_train, y_test = train_test_split(
    X_seq, y_svm, test_size=0.3, random_state=42, stratify=y
)

print("Train shape :", X_train.shape)
print("Test shape :", X_test.shape)
print("\n" + "="*60 + "\n")


# =============================
# 9) Construction du mod√®le GRU-SVM
# =============================
from tensorflow.keras.layers import Dropout

gru_units = 128
inputs = Input(shape=(30, 1))
x = layers.GRU(gru_units, return_sequences=False,
               kernel_regularizer=regularizers.l2(1e-4))(inputs)
x = Dropout(0.5)(x)
outputs = layers.Dense(1, kernel_regularizer=regularizers.l2(1e-4))(x)

model = Model(inputs, outputs)
model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),
              loss=SquaredHinge())

history = model.fit(X_train, y_train,
                    epochs=3000,
                    batch_size=128,
                    validation_split=0.1,
                    verbose=2)

model.summary()



# =============================
# 12) √âvaluation
# =============================
print("\n=== √âVALUATION ===")

# pr√©dictions (score SVM brut)
y_pred_raw = model.predict(X_test).ravel()

# labels +1 / -1
y_pred = np.where(y_pred_raw >= 0, 1, -1)

# conversion pour m√©triques (0/1)
y_test_binary = np.where(y_test == 1, 1, 0)
y_pred_binary = np.where(y_pred == 1, 1, 0)

# Accuracy
acc = accuracy_score(y_test_binary, y_pred_binary)
print("Accuracy :", acc)

# Confusion matrix
cm = confusion_matrix(y_test_binary, y_pred_binary)
print("\nMatrice de confusion :\n", cm)

# Sensibilit√© / Sp√©cificit√©
tn, fp, fn, tp = cm.ravel()
sensitivity = tp / (tp + fn)
specificity = tn / (tn + fp)

print("\nSensitivity (Recall Malignant):", sensitivity)
print("Specificity (Benign):", specificity)

# Rapport complet
print("\nClassification Report :\n")
print(classification_report(y_test_binary, y_pred_binary))

####run------------------------------------------------

# ============================================
# CELLULE CRITIQUE √Ä AJOUTER √Ä LA FIN DE TON NOTEBOOK
# Pour sauvegarder le mod√®le MLP pour Flask
# ============================================

print("=" * 60)
print("EXPORTATION DU MOD√àLE MLP POUR L'APPLICATION FLASK")
print("=" * 60)

import joblib
import json
import os
from tensorflow.keras.models import save_model

# 1. Cr√©er le dossier models s'il n'existe pas
os.makedirs('models', exist_ok=True)
print("üìÅ Dossier 'models/' cr√©√©")

# 2. Sauvegarder le StandardScaler (TR√àS IMPORTANT !)
# Assure-toi que 'scaler' existe dans ton notebook
try:
    joblib.dump(scaler, 'models/scaler.pkl')
    print("‚úÖ StandardScaler sauvegard√©: models/scaler.pkl")
except:
    print("‚ùå ERREUR: Variable 'scaler' non trouv√©e")
    print("   Ton notebook doit avoir: scaler = StandardScaler()")

# 3. Sauvegarder le mod√®le MLP (LE PLUS IMPORTANT !)
# Assure-toi que ton mod√®le MLP s'appelle 'model' ou change le nom
try:
    # SI ton mod√®le s'appelle 'model' dans le notebook:
    save_model(model, 'models/mlp_model.h5')
    print("‚úÖ Mod√®le MLP sauvegard√©: models/mlp_model.h5")
    
    # SI ton mod√®le a un autre nom, d√©commente la ligne correspondante:
    # save_model(model_mlp, 'models/mlp_model.h5')  # si c'est model_mlp
    # save_model(mlp, 'models/mlp_model.h5')        # si c'est mlp
    # save_model(mlp_model, 'models/mlp_model.h5')  # si c'est mlp_model
    
except Exception as e:
    print(f"‚ùå ERREUR avec le mod√®le MLP: {e}")
    print("   V√©rifie le nom de ta variable mod√®le MLP")

# 4. Sauvegarder les noms des 30 features (ESSENTIEL pour l'interface)
try:
    feature_names = X_scaled.columns.tolist()  # X_scaled doit exister
    with open('models/feature_names.json', 'w') as f:
        json.dump(feature_names, f)
    print(f"‚úÖ 30 noms de features sauvegard√©s: models/feature_names.json")
    print(f"   Exemple: {feature_names[:3]}...")
except:
    print("‚ùå ERREUR: X_scaled non trouv√©")
    print("   Ton notebook doit avoir: X_scaled = StandardScaler().fit_transform(X)")

# 5. V√©rifier les fichiers cr√©√©s
print("\n" + "=" * 60)
print("V√âRIFICATION DES FICHIERS CR√â√âS:")
print("=" * 60)

if os.path.exists('models'):
    fichiers = os.listdir('models')
    for f in fichiers:
        taille = os.path.getsize(f'models/{f}') / 1024  # en Ko
        print(f"üìÑ {f} ({taille:.1f} Ko)")
    
    if len(fichiers) >= 3:
        print(f"\nüéâ SUCC√àS! {len(fichiers)} fichiers cr√©√©s.")
        print("   Ton application Flask est pr√™te!")
    else:
        print(f"\n‚ö†Ô∏è  ATTENTION: Seulement {len(fichiers)} fichiers")
        print("   V√©rifie les erreurs ci-dessus")
else:
    print("‚ùå CRITIQUE: Dossier 'models/' non cr√©√©!")

print("\n" + "=" * 60)
print("PROCHAINES √âTAPES:")
print("1. Copie app.py et templates/index.html")
print("2. Lance: python app.py")
print("3. Ouvre: http://localhost:5000")
print("=" * 60)